{
 "cells": [
  {
   "cell_type": "code"perform_speaker_diarization
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"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\")\n",
    "\n",
    "# send pipeline to GPU (when available)\n",
    "import torch\n",
    "#pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(\"wav/a.wav\")\n",
    "\n",
    "# print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "# start=0.2s stop=1.5s speaker_0\n",
    "# start=1.8s stop=3.9s speaker_1\n",
    "# start=4.2s stop=5.7s speaker_0\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "\n",
    "        elif text and len(text) > 0 and text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text\n",
    "\n",
    "\n",
    "def diarize_text(transcribe_res, diarization_result):\n",
    "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "    res_processed = merge_sentence(spk_text)\n",
    "    return res_processed\n",
    "\n",
    "\n",
    "def write_to_txt(spk_sent, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for seg, spk, sentence in spk_sent:\n",
    "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "asr_result = model.transcribe(\"wav/a.m4a\")\n",
    "final_result = diarize_text(asr_result, diarization)\n",
    "\n",
    "for seg, spk, sent in final_result:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyannote.audio\n",
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"numpy<=2.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "from pyannote.core import Segment, Annotation\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def process_audio_array(waveform):\n",
    "    \"\"\"\n",
    "    Process audio bytes to perform diarization and speech-to-text, returning a dictionary\n",
    "    with speaker segments and transcriptions.\n",
    "    \n",
    "    Args:\n",
    "        audio_bytes (bytes): Audio data as bytes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with segments containing start time, end time, speaker, and text\n",
    "    \"\"\"\n",
    "    # Create BytesIO object from audio bytes\n",
    "    \n",
    "    # Initialize diarization pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"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\"\n",
    "    )\n",
    "    \n",
    "    # Apply diarization (using the BytesIO object)\n",
    "    diarization = pipeline({\n",
    "        \"waveform\": torch.tensor(waveform).unsqueeze(0),\n",
    "        \"sample_rate\": 16000\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Initialize speech recognition model\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    \n",
    "    # Transcribe audio (using the same BytesIO object)\n",
    "    asr_result = model.transcribe(waveform)\n",
    "    \n",
    "    # Process results\n",
    "    timestamp_texts = get_text_with_timestamp(asr_result)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization)\n",
    "    final_result = merge_sentence(spk_text)\n",
    "    \n",
    "    # Convert to dictionary format\n",
    "    result_dict = {\n",
    "        \"segments\": []\n",
    "    }\n",
    "    \n",
    "    for seg, spk, text in final_result:\n",
    "        segment = {\n",
    "            \"start\": round(seg.start, 2),\n",
    "            \"end\": round(seg.end, 2),\n",
    "            \"speaker\": spk,\n",
    "            \"text\": text\n",
    "        }\n",
    "        result_dict[\"segments\"].append(segment)\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "        elif text and len(text) > 0 and text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 8.92 SPEAKER_00  Okay, so I will start the presentation and today My brother is fucking talking to you now. Anyway, so now it's your turn, bro\n",
      "8.92 18.54 SPEAKER_01  Okay, I want hamburger with fries and some ketchup Master, but I don't want people's please. Thank you\n",
      "19.36 24.52 SPEAKER_03  But do you want sorry? I'm not sure yet. What can you recommend?\n",
      "25.56 30.04 SPEAKER_03  looking for something Heavy maybe some pasta\n",
      "32.00 38.12 SPEAKER_02  Also, where is the toilet? I want a big fat burger with a lot of cheese\n",
      "39.80 41.96 SPEAKER_01  Thank you very much, and joy\n"
     ]
    }
   ],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.utils import process_audio_array as processssss\n",
    "\n",
    "from pyannote.audio import Audio, Pipeline\n",
    "\n",
    "audio = Audio()\n",
    "\n",
    "# Example usage\n",
    "file_path = \"wav/a.wav\"\n",
    "waveform = whisper.audio.load_audio(file_path)\n",
    "results = processssss(waveform)\n",
    "\n",
    "    \n",
    "# Print the results\n",
    "for segment in results[\"segments\"]:\n",
    "    print(f\"{segment['start']:.2f} {segment['end']:.2f} {segment['speaker']} {segment['text']}\")\n",
    "\n",
    "# Optionally save results to a file\n",
    "with open(\"diarization_results.txt\", 'w') as f:\n",
    "    for segment in results[\"segments\"]:\n",
    "        f.write(f\"{segment['start']:.2f} {segment['end']:.2f} {segment['speaker']} {segment['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speakers_chunks(segments_results):\n",
    "    # Create a dictionary to hold the speaker's chunks\n",
    "    speaker_chunks = {}\n",
    "\n",
    "    sampling_rate = 16000\n",
    "    # Process the segments to group them by speaker and chunk them\n",
    "    for segment in segments_results['segments']:\n",
    "        speaker = segment['speaker']\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        \n",
    "        # Calculate start and end sample indices based on the sampling rate\n",
    "        start_sample = int(start * sampling_rate)\n",
    "        end_sample = int(end * sampling_rate)\n",
    "        \n",
    "        # Extract the chunk from the waveform\n",
    "        chunk = waveform[start_sample:end_sample]\n",
    "        \n",
    "        # If the speaker is already in the dictionary, append the chunk to their list, else create a new entry\n",
    "        if speaker in speaker_chunks:\n",
    "            speaker_chunks[speaker].append(chunk)\n",
    "        else:\n",
    "            speaker_chunks[speaker] = [chunk]\n",
    "\n",
    "    return speaker_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing chunks for SPEAKER_00:\n",
      "Playing chunk 1...\n",
      "Playing chunks for SPEAKER_01:\n",
      "Playing chunk 1...\n",
      "Playing chunk 2...\n",
      "Playing chunks for SPEAKER_03:\n",
      "Playing chunk 1...\n",
      "Playing chunk 2...\n",
      "Playing chunks for SPEAKER_02:\n",
      "Playing chunk 1...\n"
     ]
    }
   ],
   "source": [
    "def play_chunks(chunks):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Playing chunk {i+1}...\")\n",
    "        sd.play(chunk, samplerate=sampling_rate)\n",
    "        sd.wait()  # Wait for the audio to finish before playing the next chunk\n",
    "\n",
    "# Play all chunks for each speaker\n",
    "for speaker, chunks in speaker_chunks.items():\n",
    "    print(f\"Playing chunks for {speaker}:\")\n",
    "    play_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
