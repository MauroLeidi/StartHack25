{
 "cells": [
  {
   "cell_type": "code"perform_speaker_diarization
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"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\")\n",
    "\n",
    "# send pipeline to GPU (when available)\n",
    "import torch\n",
    "#pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(\"wav/a.wav\")\n",
    "\n",
    "# print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "# start=0.2s stop=1.5s speaker_0\n",
    "# start=1.8s stop=3.9s speaker_1\n",
    "# start=4.2s stop=5.7s speaker_0\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "\n",
    "        elif text and len(text) > 0 and text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text\n",
    "\n",
    "\n",
    "def diarize_text(transcribe_res, diarization_result):\n",
    "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "    res_processed = merge_sentence(spk_text)\n",
    "    return res_processed\n",
    "\n",
    "\n",
    "def write_to_txt(spk_sent, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for seg, spk, sentence in spk_sent:\n",
    "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "asr_result = model.transcribe(\"wav/a.m4a\")\n",
    "final_result = diarize_text(asr_result, diarization)\n",
    "\n",
    "for seg, spk, sent in final_result:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyannote.audio\n",
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"numpy<=2.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "from pyannote.core import Segment, Annotation\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def process_audio_array(waveform):\n",
    "    \"\"\"\n",
    "    Process audio bytes to perform diarization and speech-to-text, returning a dictionary\n",
    "    with speaker segments and transcriptions.\n",
    "    \n",
    "    Args:\n",
    "        audio_bytes (bytes): Audio data as bytes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with segments containing start time, end time, speaker, and text\n",
    "    \"\"\"\n",
    "    # Create BytesIO object from audio bytes\n",
    "    \n",
    "    # Initialize diarization pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"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\"\n",
    "    )\n",
    "    \n",
    "    # Apply diarization (using the BytesIO object)\n",
    "    diarization = pipeline({\n",
    "        \"waveform\": torch.tensor(waveform).unsqueeze(0),\n",
    "        \"sample_rate\": 16000\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Initialize speech recognition model\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    \n",
    "    # Transcribe audio (using the same BytesIO object)\n",
    "    asr_result = model.transcribe(waveform)\n",
    "    \n",
    "    # Process results\n",
    "    timestamp_texts = get_text_with_timestamp(asr_result)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization)\n",
    "    final_result = merge_sentence(spk_text)\n",
    "    \n",
    "    # Convert to dictionary format\n",
    "    result_dict = {\n",
    "        \"segments\": []\n",
    "    }\n",
    "    \n",
    "    for seg, spk, text in final_result:\n",
    "        segment = {\n",
    "            \"start\": round(seg.start, 2),\n",
    "            \"end\": round(seg.end, 2),\n",
    "            \"speaker\": spk,\n",
    "            \"text\": text\n",
    "        }\n",
    "        result_dict[\"segments\"].append(segment)\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "        elif text and len(text) > 0 and text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 3.00 SPEAKER_00  Hey, I'm Diego has a pizza.\n",
      "3.00 5.00 SPEAKER_00  Do you have a Diego one for pizza?\n",
      "5.00 7.00 SPEAKER_00  So how do you have a milk?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.utils import process_audio_array as processssss\n",
    "\n",
    "from pyannote.audio import Audio, Pipeline\n",
    "\n",
    "audio = Audio()\n",
    "\n",
    "# Example usage\n",
    "file_path = \"wav/input.wav\"\n",
    "waveform = whisper.audio.load_audio(file_path)\n",
    "results = processssss(waveform)\n",
    "\n",
    "    \n",
    "# Print the results\n",
    "for segment in results[\"segments\"]:\n",
    "    print(f\"{segment['start']:.2f} {segment['end']:.2f} {segment['speaker']} {segment['text']}\")\n",
    "\n",
    "# Optionally save results to a file\n",
    "with open(\"diarization_results.txt\", 'w') as f:\n",
    "    for segment in results[\"segments\"]:\n",
    "        f.write(f\"{segment['start']:.2f} {segment['end']:.2f} {segment['speaker']} {segment['text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speakers_chunks(segments_results):\n",
    "    # Create a dictionary to hold the speaker's chunks\n",
    "    speaker_chunks = {}\n",
    "\n",
    "    sampling_rate = 16000\n",
    "    # Process the segments to group them by speaker and chunk them\n",
    "    for segment in segments_results['segments']:\n",
    "        speaker = segment['speaker']\n",
    "        start = segment['start']\n",
    "        end = segment['end']\n",
    "        \n",
    "        # Calculate start and end sample indices based on the sampling rate\n",
    "        start_sample = int(start * sampling_rate)\n",
    "        end_sample = int(end * sampling_rate)\n",
    "        \n",
    "        # Extract the chunk from the waveform\n",
    "        chunk = waveform[start_sample:end_sample]\n",
    "        \n",
    "        # If the speaker is already in the dictionary, append the chunk to their list, else create a new entry\n",
    "        if speaker in speaker_chunks:\n",
    "            speaker_chunks[speaker].append(chunk)\n",
    "        else:\n",
    "            speaker_chunks[speaker] = [chunk]\n",
    "\n",
    "    return speaker_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice) (2.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing chunks for SPEAKER_00:\n",
      "Playing chunk 1...\n",
      "Playing chunks for SPEAKER_01:\n",
      "Playing chunk 1...\n",
      "Playing chunk 2...\n",
      "Playing chunks for SPEAKER_03:\n",
      "Playing chunk 1...\n",
      "Playing chunk 2...\n",
      "Playing chunks for SPEAKER_02:\n",
      "Playing chunk 1...\n"
     ]
    }
   ],
   "source": [
    "def play_chunks(chunks):\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Playing chunk {i+1}...\")\n",
    "        sd.play(chunk, samplerate=sampling_rate)\n",
    "        sd.wait()  # Wait for the audio to finish before playing the next chunk\n",
    "\n",
    "# Play all chunks for each speaker\n",
    "for speaker, chunks in speaker_chunks.items():\n",
    "    print(f\"Playing chunks for {speaker}:\")\n",
    "    play_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate_summary_persona(messages):\n",
    "    \"\"\"\n",
    "    Generate personas based on conversation data, grouping each human speaker with robot responses.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of conversation exchanges\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of speaker_ids and their generated personas\n",
    "    \"\"\"\n",
    "    # Identify all unique speakers excluding the robot\n",
    "    human_speakers = set()\n",
    "    for exchange in messages:\n",
    "        for message_obj in exchange:\n",
    "            if message_obj[\"speaker_id\"] != \"robot\":\n",
    "                human_speakers.add(message_obj[\"speaker_id\"])\n",
    "    \n",
    "    # Organize conversations by human speaker + robot responses\n",
    "    speaker_conversations = {}\n",
    "    for speaker_id in human_speakers:\n",
    "        speaker_conversations[speaker_id] = []\n",
    "    \n",
    "    # Reconstruct conversation flows for each human speaker with robot\n",
    "    robot_responses = []\n",
    "    current_speakers = set()\n",
    "    \n",
    "    for exchange in messages:\n",
    "        # Get speakers in this exchange\n",
    "        exchange_speakers = set()\n",
    "        for message_obj in exchange:\n",
    "            if message_obj[\"speaker_id\"] != \"robot\":\n",
    "                exchange_speakers.add(message_obj[\"speaker_id\"])\n",
    "        \n",
    "        # Add human messages to their respective conversations\n",
    "        for message_obj in exchange:\n",
    "            if message_obj[\"speaker_id\"] != \"robot\":\n",
    "                speaker_conversations[message_obj[\"speaker_id\"]].append({\n",
    "                    \"speaker_id\": message_obj[\"speaker_id\"],\n",
    "                    \"message\": message_obj[\"message\"]\n",
    "                })\n",
    "            else:\n",
    "                # This is a robot response - store it\n",
    "                robot_responses.append({\n",
    "                    \"speaker_id\": \"robot\",\n",
    "                    \"message\": message_obj[\"message\"],\n",
    "                    \"responding_to\": exchange_speakers  # Which speakers the robot is responding to\n",
    "                })\n",
    "                \n",
    "                # Add this robot response to all speakers from the previous exchange\n",
    "                for speaker_id in current_speakers:\n",
    "                    speaker_conversations[speaker_id].append({\n",
    "                        \"speaker_id\": \"robot\",\n",
    "                        \"message\": message_obj[\"message\"]\n",
    "                    })\n",
    "        \n",
    "        # Update current speakers for the next iteration\n",
    "        if exchange_speakers:\n",
    "            current_speakers = exchange_speakers\n",
    "    \n",
    "    # Generate personas for each human speaker\n",
    "    personas = {}\n",
    "    \n",
    "    # Load the Jinja template\n",
    "    env = Environment(loader=FileSystemLoader('.'))\n",
    "    template = env.get_template('summary_persona.jinja')\n",
    "    \n",
    "    for speaker_id, conversation in speaker_conversations.items():\n",
    "        # Render the template with the speaker-robot conversation\n",
    "        prompt = template.render(\n",
    "            speaker_id=speaker_id,\n",
    "            conversation=conversation\n",
    "        )\n",
    "        \n",
    "        response = call_openai_api(prompt)\n",
    "        personas[speaker_id] = response\n",
    "    \n",
    "\n",
    "    # save personas to csv with \n",
    "    data = []\n",
    "    for user_id, summary_persona in personas.items():\n",
    "        data.append({\n",
    "            'user_id': user_id,\n",
    "            'summary_persona': summary_persona\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame with specified columns\n",
    "    df = pd.DataFrame(data, columns=['user_id', 'summary_persona'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('summary_personas.csv', index=False)\n",
    "    return personas\n",
    "\n",
    "def call_openai_api(prompt):\n",
    "    \"\"\"Helper function to call OpenAI API\"\"\"\n",
    "    URL = \"https://api.openai.com/v1/chat/completions\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(URL, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error generating persona: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\n",
    "    \"session_id_01\": [\n",
    "        [\n",
    "            {\n",
    "                \"speaker_id\": \"speaker-00\",\n",
    "                \"message\": \"First speaker.\",\n",
    "            },\n",
    "            {\n",
    "                \"speaker_id\": \"speaker-01\",\n",
    "                \"message\": \"Second speaker in the same audio.\",\n",
    "            },\n",
    "        ],\n",
    "        [\n",
    "            {\n",
    "                \"speaker_id\": \"robot\",\n",
    "                \"message\": \"The response of the robot.\",\n",
    "            },\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "messages_session_id_01 = messages[\"session_id_01\"]\n",
    "personas = generate_summary_persona(messages_session_id_01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'speaker-00': 'Persona description for speaker ID \"speaker-00\"\\n\\n- Preferences and meal choices:\\n  - No details provided.\\n\\n- Dietary restrictions or allergies:\\n  - No details provided.\\n\\n- Meal customization:\\n  - No details provided.\\n\\n- Attitude towards food:\\n  - No details provided.\\n\\n- Eating habits:\\n  - No details provided.\\n\\n- Additional lifestyle information:\\n  - No details provided.',\n",
       " 'speaker-01': '**Persona description for speaker ID \"speaker-01\":**\\n\\n- **Preferences and meal choices:**\\n  - Typically prefers savory options for meals.\\n  - Enjoys experimenting with different international cuisines.\\n  - Likes having a range of appetizers before the main course.\\n  - Enjoys a balanced meal with a mix of proteins and greens.\\n\\n- **Dietary restrictions or allergies:**\\n  - Has a lactose intolerance; avoids dairy products or requests lactose-free alternatives.\\n  - Communicates dairy intolerance clearly and seeks available substitutions.\\n\\n- **Meal customization:**\\n  - Often requests dressing on the side for salads.\\n  - Prefers to have meals with no excess oil or butter; opts for healthier cooking styles.\\n  - Requests smaller portion sizes occasionally, indicating some mindfulness towards portion control.\\n\\n- **Attitude towards food:**\\n  - Health-conscious and mindful of nutritional content.\\n  - Emphasizes enjoying food without compromising on health.\\n  - Shows interest in foods that are both nutritious and flavorful.\\n\\n- **Eating habits:**\\n  - Has a habit of drinking water consistently throughout meals.\\n  - Preferred to have light dinners and avoids heavy meals late at night.\\n  - Frequently opts for fruit-based desserts over heavy, sugary options.\\n\\n- **Additional lifestyle information:**\\n  - Follows a fitness-oriented lifestyle, leading to choices that support physical wellness.\\n  - Shows interest in eco-friendly dining options and questions ingredients for sustainability.\\n  - Values quality dining experiences and tends to choose dining environments that offer healthy food choices.'}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.67.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Downloading openai-1.67.0-py3-none-any.whl (580 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m580.2/580.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-macosx_11_0_arm64.whl (321 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openai-1.67.0 pydantic-2.10.6 pydantic-core-2.27.2 sniffio-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Internet turned on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The internet is now turned on. What would you like to do next?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set your API key\n",
    "client = openai.OpenAI()\n",
    "\n",
    "# Define the functions that can be called\n",
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"call_internet\",  # Changed to match the function you're implementing\n",
    "            \"description\": \"Connect to the internet to get latest information\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},  # No parameters needed for this function\n",
    "                \"required\": []\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# This would be your actual implementation of the internet connection function\n",
    "def call_internet():\n",
    "    \"\"\"Function to be called when we want to connect to internet\"\"\"\n",
    "    print(\"Internet turned on\")\n",
    "    return {\"internet_status\": True}\n",
    "\n",
    "# Start conversation with the user\n",
    "messages = [{\"role\": \"user\", \"content\": \"Turn internet on\"}]\n",
    "\n",
    "# Make the API call\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=messages,\n",
    "    tools=functions,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "# Process the response\n",
    "response_message = response.choices[0].message\n",
    "messages.append(response_message)  # Add to conversation history\n",
    "\n",
    "# Check if the model wants to call a function\n",
    "if response_message.tool_calls:\n",
    "    # Loop through all function calls the model wants to make\n",
    "    for tool_call in response_message.tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        \n",
    "        # Call the appropriate function\n",
    "        if function_name == \"call_internet\":\n",
    "            function_response = call_internet()\n",
    "            \n",
    "            # Add the function response to the messages\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": tool_call.id,  # This needs to match the ID from the model\n",
    "                \"name\": function_name,\n",
    "                \"content\": json.dumps(function_response)\n",
    "            })\n",
    "\n",
    "    # Get a new response from the model with function results\n",
    "    second_response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    print(second_response.choices[0].message.content)\n",
    "else:\n",
    "    # Model didn't call a function\n",
    "    print(response_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
