{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"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\")\n",
    "\n",
    "# send pipeline to GPU (when available)\n",
    "import torch\n",
    "#pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(\"wav/a.wav\")\n",
    "\n",
    "# print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "# start=0.2s stop=1.5s speaker_0\n",
    "# start=1.8s stop=3.9s speaker_1\n",
    "# start=4.2s stop=5.7s speaker_0\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Segment, Annotation, Timeline\n",
    "\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "\n",
    "        elif text and len(text) > 0 and text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text\n",
    "\n",
    "\n",
    "def diarize_text(transcribe_res, diarization_result):\n",
    "    timestamp_texts = get_text_with_timestamp(transcribe_res)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization_result)\n",
    "    res_processed = merge_sentence(spk_text)\n",
    "    return res_processed\n",
    "\n",
    "\n",
    "def write_to_txt(spk_sent, file):\n",
    "    with open(file, 'w') as fp:\n",
    "        for seg, spk, sentence in spk_sent:\n",
    "            line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sentence}\\n'\n",
    "            fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"tiny.en\")\n",
    "asr_result = model.transcribe(\"wav/a.m4a\")\n",
    "final_result = diarize_text(asr_result, diarization)\n",
    "\n",
    "for seg, spk, sent in final_result:\n",
    "    line = f'{seg.start:.2f} {seg.end:.2f} {spk} {sent}'\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyannote.audio\n",
    "!pip install -U openai-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"numpy<=2.1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "from pyannote.core import Segment, Annotation\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def process_audio_array(waveform):\n",
    "    \"\"\"\n",
    "    Process audio bytes to perform diarization and speech-to-text, returning a dictionary\n",
    "    with speaker segments and transcriptions.\n",
    "    \n",
    "    Args:\n",
    "        audio_bytes (bytes): Audio data as bytes\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with segments containing start time, end time, speaker, and text\n",
    "    \"\"\"\n",
    "    # Create BytesIO object from audio bytes\n",
    "    \n",
    "    # Initialize diarization pipeline\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=\"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\"\n",
    "    )\n",
    "    \n",
    "    # Apply diarization (using the BytesIO object)\n",
    "    diarization = pipeline({\n",
    "        \"waveform\": torch.tensor(waveform).unsqueeze(0),\n",
    "        \"sample_rate\": 16000\n",
    "    })\n",
    "\n",
    "    \n",
    "    # Initialize speech recognition model\n",
    "    model = whisper.load_model(\"tiny.en\")\n",
    "    \n",
    "    # Transcribe audio (using the same BytesIO object)\n",
    "    asr_result = model.transcribe(waveform)\n",
    "    \n",
    "    # Process results\n",
    "    timestamp_texts = get_text_with_timestamp(asr_result)\n",
    "    spk_text = add_speaker_info_to_text(timestamp_texts, diarization)\n",
    "    final_result = merge_sentence(spk_text)\n",
    "    \n",
    "    # Convert to dictionary format\n",
    "    result_dict = {\n",
    "        \"segments\": []\n",
    "    }\n",
    "    \n",
    "    for seg, spk, text in final_result:\n",
    "        segment = {\n",
    "            \"start\": round(seg.start, 2),\n",
    "            \"end\": round(seg.end, 2),\n",
    "            \"speaker\": spk,\n",
    "            \"text\": text\n",
    "        }\n",
    "        result_dict[\"segments\"].append(segment)\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "def get_text_with_timestamp(transcribe_res):\n",
    "    timestamp_texts = []\n",
    "    for item in transcribe_res['segments']:\n",
    "        start = item['start']\n",
    "        end = item['end']\n",
    "        text = item['text']\n",
    "        timestamp_texts.append((Segment(start, end), text))\n",
    "    return timestamp_texts\n",
    "\n",
    "def add_speaker_info_to_text(timestamp_texts, ann):\n",
    "    spk_text = []\n",
    "    for seg, text in timestamp_texts:\n",
    "        spk = ann.crop(seg).argmax()\n",
    "        spk_text.append((seg, spk, text))\n",
    "    return spk_text\n",
    "\n",
    "def merge_cache(text_cache):\n",
    "    sentence = ''.join([item[-1] for item in text_cache])\n",
    "    spk = text_cache[0][1]\n",
    "    start = text_cache[0][0].start\n",
    "    end = text_cache[-1][0].end\n",
    "    return Segment(start, end), spk, sentence\n",
    "\n",
    "PUNC_SENT_END = ['.', '?', '!']\n",
    "\n",
    "def merge_sentence(spk_text):\n",
    "    merged_spk_text = []\n",
    "    pre_spk = None\n",
    "    text_cache = []\n",
    "    for seg, spk, text in spk_text:\n",
    "        if spk != pre_spk and pre_spk is not None and len(text_cache) > 0:\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = [(seg, spk, text)]\n",
    "            pre_spk = spk\n",
    "        elif text and len(text) > 0 and text[-1] in PUNC_SENT_END:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            merged_spk_text.append(merge_cache(text_cache))\n",
    "            text_cache = []\n",
    "            pre_spk = spk\n",
    "        else:\n",
    "            text_cache.append((seg, spk, text))\n",
    "            pre_spk = spk\n",
    "    if len(text_cache) > 0:\n",
    "        merged_spk_text.append(merge_cache(text_cache))\n",
    "    return merged_spk_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/Users/gioelemonopoli/miniconda3/envs/hack25/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 4.00 SPEAKER_02  Okay, so this is a test here with my already.\n",
      "4.00 6.00 SPEAKER_02  My only problem is...\n",
      "6.00 8.00 SPEAKER_01  I don't know.\n",
      "8.00 10.00 SPEAKER_01  What do you like?\n",
      "10.00 12.00 SPEAKER_03  I'll speak a bit later.\n",
      "12.00 19.00 SPEAKER_01  Okay, I'm here at the plant presentation.\n"
     ]
    }
   ],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.utils import process_audio_array as processssss\n",
    "\n",
    "from pyannote.audio import Audio, Pipeline\n",
    "\n",
    "audio = Audio()\n",
    "\n",
    "# Example usage\n",
    "file_path = \"wav/mauro_gioele_recording.wav\"\n",
    "waveform = whisper.audio.load_audio(file_path)\n",
    "results = processssss(waveform)\n",
    "\n",
    "    \n",
    "# Print the results\n",
    "for segment in results[\"segments\"]:\n",
    "    print(f\"{segment['start']:.2f} {segment['end']:.2f} {segment['speaker']} {segment['text']}\")\n",
    "\n",
    "# Optionally save results to a file\n",
    "with open(\"diarization_results.txt\", 'w') as f:\n",
    "    for segment in results[\"segments\"]:\n",
    "        f.write(f\"{segment['start']:.2f} {segment['end']:.2f} {segment['speaker']} {segment['text']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
